# -*- coding: utf-8 -*-
"""motion_track.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/107YOspW23HmmPuhahz9Cow_0G6BSe6y0
"""

import cv2
import numpy as np
import neoapi
import time
import numpy as np
import pandas as pd


camera = neoapi.Cam()  # Create camera object
camera.Connect('700006260959')  # Connect to camera 959 (cryo)
camera.f.ExposureTime.Set(45)  # Set exposure time
camera.f.AcquisitionFrameRateEnable.value = True
camera.f.AcquisitionFrameRate.value = 5000 #fps



def main():
    """
    Main method of the program.
    """
    x_cord=[]
    y_cord=[]
    fps_time=[]
    # Create a VideoCapture object


    # Create the background subtractor object
    # Use the last 700 video frames to build the background
    back_sub = cv2.createBackgroundSubtractorMOG2(history=700,
        varThreshold=25, detectShadows=True)

    # Create kernel for morphological operation
    # You can tweak the dimensions of the kernel
    # e.g. instead of 20,20 you can try 30,30.
    kernel = np.ones((1,1),np.uint8)

    for i in range(10000):
        start_time = time.time() #start time for fps calculation
        # Capture frame-by-frame

        img = camera.GetImage()
        frame = img.GetNPArray()

        text1 = "q to quit"
        position = (10, 30)  # Adjust the coordinates as needed

        # Choose font and scale
        font = cv2.FONT_HERSHEY_SIMPLEX
        font_scale = 1
        font_thickness = 1
        font_color = (255, 255, 255)  # White color

        # Add text to the image
        #cv2.putText(frame, text1, position, font, font_scale, font_color, font_thickness)


        # Use every frame to calculate the foreground mask and update
        # the background
        fg_mask = back_sub.apply(frame)

        # Close dark gaps in foreground object using closing
        fg_mask = cv2.morphologyEx(fg_mask, cv2.MORPH_CLOSE, kernel)

        # Remove salt and pepper noise with a median filter
        fg_mask = cv2.medianBlur(fg_mask, 5)

        # Threshold the image to make it either black or white
        _, fg_mask = cv2.threshold(fg_mask,127,255,cv2.THRESH_BINARY)

        # Find the index of the largest contour and draw bounding box
        fg_mask_bb = fg_mask
        contours, hierarchy = cv2.findContours(fg_mask_bb,cv2.RETR_TREE,cv2.CHAIN_APPROX_SIMPLE)[-2:]
        areas = [cv2.contourArea(c) for c in contours]

        # If there are no countours
        if len(areas) < 1:

            # Display the resulting frame
            #cv2.imshow('ceylon',frame)

            # If "q" is pressed on the keyboard,
            # exit this loop
            #if cv2.waitKey(1) & 0xFF == ord('q'):
            #    break

            # Go to the top of the while loop
            continue

        else:
            # Find the largest moving object in the image
            max_index = np.argmax(areas)

        # Draw the bounding box
        cnt = contours[max_index]
        x,y,w,h = cv2.boundingRect(cnt)
        #cv2.rectangle(frame,(x,y),(x+w,y+h),(0,255,0),3)

        # Draw circle in the center of the bounding box
        x2 = x + int(w/2)
        y2 = y + int(h/2)
        #cv2.circle(frame,(x2,y2),4,(0,255,0),-1)

        # Print the centroid coordinates (we'll use the center of the
        # bounding box) on the image
        text = "x: " + str(x2) + ", y: " + str(y2)
        #cv2.putText(frame, text, (x2 - 10, y2 - 10),
        #    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

        #Implementation of time taken for the while loop to execute in ms for 4 decimal point accuracy

        end_time = time.time()
        del_time=(end_time-start_time)*1000
        del_time_str = f"{del_time:.4f} in ms"
        #cv2.putText(frame, del_time_str, (30, 50),
        #    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)
        #cv2.imshow('ceylon',frame) # Display the resulting frame


        # If "q" is pressed on the keyboard, bbb
        # exit this loop
        #if cv2.waitKey(1) & 0xFF == ord('q'):
        #    break
        x_cord.append(x2)
        y_cord.append(y2)
        fps_time.append(del_time)


    # Close down the video stream
    camera.Disconnect()
    cv2.destroyAllWindows()
    #saving the coordinate file
    data = {'x_cord': x_cord, 'y_cord': y_cord, 'del_time': del_time}
    df=pd.DataFrame(data)
    df.reset_index(inplace=True)
    df.to_csv('data_vib1.csv')
if __name__ == '__main__':
    print(__doc__)
    main()

# Reference: C. Mudiyans, GitLab, 2024.
